{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0fe7999-f7a2-4e89-9e75-ededa2a96df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy import signal\n",
    "from scipy.ndimage import median_filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33361eb3-a188-4d8d-9812-14d468099ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_csv(\"hacktrain.csv\")\n",
    "test_df = pd.read_csv(\"hacktest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e7d8cd-081f-4ba6-be3a-682f48176352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "if 'Unnamed: 0' in train_df.columns:\n",
    "    train_df = train_df.drop(\"Unnamed: 0\", axis=1)\n",
    "if 'Unnamed: 0' in test_df.columns:\n",
    "    test_df = test_df.drop(\"Unnamed: 0\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd6bdfc4-a8d6-4748-b0ab-32b733cf181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 NDVI time series features\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [col for col in train_df.columns if col not in ['ID', 'class']]\n",
    "print(f\"Found {len(feature_cols)} NDVI time series features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4375c8-ce54-46ef-abcc-70cbfc1b4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X_train_full = train_df[feature_cols]\n",
    "y_train_full = train_df['class']\n",
    "X_test = test_df[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26236d5-7344-463f-903e-ad3f937c38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DENOISING TECHNIQUES\n",
    "def advanced_denoising(df, feature_cols):\n",
    "    \"\"\"Apply multiple denoising techniques for robust noise reduction\"\"\"\n",
    "    df_denoised = df.copy()\n",
    "    \n",
    "    # 1. Median filter for outlier removal (works well for salt-and-pepper noise)\n",
    "    for col in feature_cols:\n",
    "        # Apply median filter with window size 3\n",
    "        df_denoised[col] = median_filter(df[col].fillna(df[col].median()), size=3)\n",
    "    \n",
    "    # 2. Savitzky-Golay filter for temporal smoothing\n",
    "    for idx in df_denoised.index:\n",
    "        row_values = df_denoised.loc[idx, feature_cols].values\n",
    "        if not np.isnan(row_values).all():\n",
    "            # Fill NaN values temporarily for filtering\n",
    "            row_filled = pd.Series(row_values).fillna(method='ffill').fillna(method='bfill').values\n",
    "            try:\n",
    "                # Apply Savitzky-Golay filter (polynomial order 2, window length 5)\n",
    "                if len(row_filled) >= 5:\n",
    "                    smoothed = signal.savgol_filter(row_filled, window_length=5, polyorder=2)\n",
    "                    df_denoised.loc[idx, feature_cols] = smoothed\n",
    "            except:\n",
    "                pass  # Keep original values if filtering fails\n",
    "    \n",
    "    return df_denoised\n",
    "\n",
    "# Apply advanced denoising\n",
    "X_train_denoised = advanced_denoising(X_train_full, feature_cols)\n",
    "X_test_denoised = advanced_denoising(X_test, feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd6d15b-da19-4ef3-9d40-2a9ba57c9f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced denoising and imputation complete.\n"
     ]
    }
   ],
   "source": [
    "#  imputation \n",
    "# Use median for robustness against outliers\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_denoised), \n",
    "                              columns=feature_cols, index=X_train_denoised.index)\n",
    "X_test_imputed = pd.DataFrame(imputer.transform(X_test_denoised), \n",
    "                             columns=feature_cols, index=X_test_denoised.index)\n",
    "\n",
    "print(\"Advanced denoising and imputation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f5d068-31ae-47bf-b06c-742acdf8f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete. Total features: 43\n"
     ]
    }
   ],
   "source": [
    "# Feature Enginnering\n",
    "def create_comprehensive_features(df, feature_cols):\n",
    "    \"\"\"Create comprehensive features for temporal NDVI analysis\"\"\"\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Basic statistical features\n",
    "    df_featured['ndvi_mean'] = df[feature_cols].mean(axis=1)\n",
    "    df_featured['ndvi_std'] = df[feature_cols].std(axis=1)\n",
    "    df_featured['ndvi_max'] = df[feature_cols].max(axis=1)\n",
    "    df_featured['ndvi_min'] = df[feature_cols].min(axis=1)\n",
    "    df_featured['ndvi_range'] = df_featured['ndvi_max'] - df_featured['ndvi_min']\n",
    "    df_featured['ndvi_median'] = df[feature_cols].median(axis=1)\n",
    "    \n",
    "    # Advanced statistical features\n",
    "    df_featured['ndvi_q25'] = df[feature_cols].quantile(0.25, axis=1)\n",
    "    df_featured['ndvi_q75'] = df[feature_cols].quantile(0.75, axis=1)\n",
    "    df_featured['ndvi_iqr'] = df_featured['ndvi_q75'] - df_featured['ndvi_q25']\n",
    "    df_featured['ndvi_skew'] = df[feature_cols].skew(axis=1)\n",
    "    df_featured['ndvi_kurt'] = df[feature_cols].kurtosis(axis=1)\n",
    "    \n",
    "    # Temporal trend features\n",
    "    time_series_data = df[feature_cols].values\n",
    "    \n",
    "    # Linear trend slope\n",
    "    trends = []\n",
    "    for i in range(len(time_series_data)):\n",
    "        x = np.arange(len(feature_cols))\n",
    "        y = time_series_data[i]\n",
    "        valid_idx = ~np.isnan(y)\n",
    "        if np.sum(valid_idx) > 1:\n",
    "            slope = np.polyfit(x[valid_idx], y[valid_idx], 1)[0]\n",
    "        else:\n",
    "            slope = 0\n",
    "        trends.append(slope)\n",
    "    df_featured['ndvi_trend'] = trends\n",
    "    \n",
    "    # Seasonal variation (assuming roughly monthly data)\n",
    "    seasonal_std = []\n",
    "    for i in range(len(time_series_data)):\n",
    "        y = time_series_data[i]\n",
    "        valid_y = y[~np.isnan(y)]\n",
    "        if len(valid_y) > 3:\n",
    "            # Calculate seasonal variation as std of detrended series\n",
    "            if len(valid_y) >= 4:\n",
    "                detrended = signal.detrend(valid_y)\n",
    "                seasonal_std.append(np.std(detrended))\n",
    "            else:\n",
    "                seasonal_std.append(np.std(valid_y))\n",
    "        else:\n",
    "            seasonal_std.append(0)\n",
    "    df_featured['ndvi_seasonal_var'] = seasonal_std\n",
    "    \n",
    "    # Vegetation vigor features (specific to NDVI)\n",
    "    df_featured['ndvi_vigor'] = df_featured['ndvi_mean'] * (1 - df_featured['ndvi_std'])\n",
    "    df_featured['ndvi_stability'] = 1 / (1 + df_featured['ndvi_std'])\n",
    "    \n",
    "    # Peak detection features\n",
    "    peak_counts = []\n",
    "    for i in range(len(time_series_data)):\n",
    "        y = time_series_data[i]\n",
    "        valid_y = y[~np.isnan(y)]\n",
    "        if len(valid_y) > 5:\n",
    "            peaks, _ = signal.find_peaks(valid_y, height=np.mean(valid_y))\n",
    "            peak_counts.append(len(peaks))\n",
    "        else:\n",
    "            peak_counts.append(0)\n",
    "    df_featured['ndvi_peak_count'] = peak_counts\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "# Apply comprehensive feature engineering\n",
    "X_train_featured = create_comprehensive_features(X_train_imputed, feature_cols)\n",
    "X_test_featured = create_comprehensive_features(X_test_imputed, feature_cols)\n",
    "\n",
    "print(f\"Feature engineering complete. Total features: {X_train_featured.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c51df623-c33b-4e6b-b783-ad48abb2fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling and Encoding\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_featured)\n",
    "X_test_scaled = scaler.transform(X_test_featured)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9454f74e-7095-4c62-938b-3f9360968394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "final_model = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    multi_class='ovr',\n",
    "    C=1.0,  # Slightly reduced regularization for better generalization\n",
    "    penalty='l1',  # L1 regularization for feature selection\n",
    "    random_state=42,\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    max_iter=2000,  # Increased iterations for convergence\n",
    "    tol=1e-6  # Tighter tolerance for better convergence\n",
    ")\n",
    "\n",
    "# Train on full training data\n",
    "final_model.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "print(\"Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd86395-9d79-478d-b399-694e1299d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.91125  0.910625 0.91125  0.91875  0.9125  ]\n",
      "Mean CV accuracy: 0.9129 (+/- 0.0060)\n"
     ]
    }
   ],
   "source": [
    "# Cross validation assessment\n",
    "# Perform stratified k-fold cross-validation on training data only\n",
    "cv_scores = cross_val_score(\n",
    "    final_model, X_train_scaled, y_train_encoded, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdf024de-7e0e-4f8f-b8c4-4a0fafcbc8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated for all 2845 samples in hacktest.csv\n"
     ]
    }
   ],
   "source": [
    "# Make predictions test set\n",
    "final_predictions_encoded = final_model.predict(X_test_scaled)\n",
    "final_predictions = label_encoder.inverse_transform(final_predictions_encoded)\n",
    "\n",
    "# Create submission file\n",
    "final_submission_df = pd.DataFrame({\n",
    "    'ID': test_df['ID'], \n",
    "    'class': final_predictions\n",
    "})\n",
    "final_submission_df.to_csv('enhanced_final_submission3.csv', index=False)\n",
    "\n",
    "print(f\"Predictions generated for all {len(test_df)} samples in hacktest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e00db67-3d67-4765-b554-94cff488395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction confidence statistics:\n",
      "Mean confidence: 0.8009\n",
      "Min confidence: 0.2759\n",
      "Max confidence: 1.0000\n",
      "Std confidence: 0.2005\n",
      "\n",
      "Prediction distribution on hacktest.csv:\n",
      "forest: 1377 (48.4%)\n",
      "farm: 628 (22.1%)\n",
      "impervious: 397 (14.0%)\n",
      "grass: 252 (8.9%)\n",
      "water: 126 (4.4%)\n",
      "orchard: 65 (2.3%)\n"
     ]
    }
   ],
   "source": [
    "# Prediction confidenece\n",
    "# Get prediction probabilities for the test set\n",
    "prediction_probs = final_model.predict_proba(X_test_scaled)\n",
    "max_probs = np.max(prediction_probs, axis=1)\n",
    "\n",
    "print(f\"Prediction confidence statistics:\")\n",
    "print(f\"Mean confidence: {max_probs.mean():.4f}\")\n",
    "print(f\"Min confidence: {max_probs.min():.4f}\")\n",
    "print(f\"Max confidence: {max_probs.max():.4f}\")\n",
    "print(f\"Std confidence: {max_probs.std():.4f}\")\n",
    "\n",
    "# Count predictions by class\n",
    "class_counts = pd.Series(final_predictions).value_counts()\n",
    "print(f\"\\nPrediction distribution on hacktest.csv:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"{class_name}: {count} ({count/len(final_predictions)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e4b68-528f-4dfc-963a-7bf528ead1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
